---
title: "Can We Really Trust the 10-Day Forecast?"
author: "Alex Baburnic, Nate Hansen, & David Vanegas"
date: "4/21/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#Forecasts
SLC_Forecast <- read.csv("forecast_data/SLC_forecast.csv")
KSL_Forecast <- read.csv("forecast_data/KSL_forecast.csv")
NY_Forecast <- read.csv("forecast_data/NY_forecast.csv")
CHI_Forecast <- read.csv("forecast_data/CHI_forecast.csv")
ATX_Forecast <- read.csv("forecast_data/ATX_forecast.csv")
SF_Forecast <- read.csv("forecast_data/SF_forecast.csv")
#Actual temperatures
SLC_Actual <- read.csv("actualTemp_data/SLC_ActualTemp.csv")
NY_Actual <- read.csv("actualTemp_data/NY_ActualTemp.csv")
CH_Actual <- read.csv("actualTemp_data/CH_ActualTemp.csv")
ATX_Actual <- read.csv("actualTemp_data/ATX_ActualTemp.csv")
SF_Actual <- read.csv("actualTemp_data/SF_ActualTemp.csv")
```

## Introduction

Anyone who has sat through a rainy picnic knows by hard experience that the reliability of the weather forecast has measurable impact on one's life. In the modern age of technology and the "Internet of Things", humans have devised algorithms to provide themselves peace-of-mind and certainty. Yet, as they eat their soggy sandwiches under a nearby pergola, people wonder why the weather forecast said it would be clear when they planned the picnic a week ago. Is it common for the forecast to be so inconsistent, or did they just get anomalously unlucky? Does the local weather station or the national one provide more accurate forecast data? Sometimes the forecast calls for relatively warm days within a week, and when the days approach, the forecast has gotten much cooler or warmer. In the Salt Lake Valley, the temperature often changes sporadically near the spring equinox, referred to by the locals as "Utah weather". Is this oscillation really unique to Utah at this time of year? Are there other cities in the United States that seem to have more reliable forecasts? The authors will attempt to answer some of these questions with confidence using statistical analysis of weather data gathered from local and national forecasts.

#### Importance of Studying Weather Station Reliability
Reliable weather forecasts would bring greater certainty to the conditions from events as small as the family picnic to as large as community firework shows or rocket launches. Although gathering data about the reliability of the forecast will not inherently improve its reliability, this information can help people make decisions about where to live. Knowing the comparative reliability of the forecast for various cities can help organizations plan large national events and assess the risk of unpredictability. Data comparing local forecasts to national forecasts will help people determine which forecast to trust more when they seem to be disparate. In a broader sense, some believe that modern extreme weather patterns are a direct result of climate change, and therefore climate change damages weather predictability. Based on these topics of value, the following questions can be operationalized:

1) In Salt Lake City, is the 7th day of the local KSL weather forecast more or less accurate than the 7th day of the forecast provided by the U. S. National Weather Service?

2) Of the cities Salt Lake City, UT; Austin, TX; San Francisco, CA; New York, NY; and Chicago, IL; which city's 10th forecast day is, on average, closest to the actual temperature that day?

3) Does the city with the "most accurate" forecast according to question 2 also have the least variability in temperature over the month of March?

***4) Do the cities with the "less accurate" forecast according to question 2 also exhibit the greatest change in average temperature or temperature variance since 80 years ago?***

*the above header line can be deleted to make this all one part of the intro.*
*List out the main topics and sections of the report to the reader, then transition to the next section. Remember, the professor wants these reports to "flow" together like a story.*

#### Methodology

To gather data that answers our question, we will use a variety of sources. Data for national forecasts will be gathered from The Weather Channel. Local Salt Lake City forecasts will be gathered from KSL Weather. Historical true high and low temperatures will be gathered from NOAA, where the stations gathering measurements will be located at the metropolis centers where the forecast is being calculated. Historic data will be gathered at only one station in each city.

The forecast sources do not seem to disclose how often they update their forecasts, so we had to devise a good way to collect a large sample of forecast predictions for each day. We considered collecting a random sample, but it is a little unrealistic to manually record the forecast at random times during the day.



## Data Collection

Display here the results of the gathered data.

Limitations:
- Forecast data was collected systematically, sampled infrequently
- Forecast sample size is relatively large, but not enough to overcome skewness
- We don't know which stations weather.com uses
- 

NOAA:
- Actual Temps 3/2/2022-4/7/2022 for SLC, Austin, TX; SF, CA; CHI, IL; NY, NY;
***-Historical data from same cities during same time***

#### Data Observations
This is an optional subsection, but it may make the document flow better. Here make some very general observations regarding the data, such as very obvious trends or outliers that may or may not be taken into consideration (and explain why).

## Data Analysis

Make meaningful analysis of the data with respect to the question of the topic. Unlike the previous subsection, go much more into detail here. Explain what certain data trends and values relate to the question and possibly make inferences that could be understood outside of statistics (you can think of the starting paragraph discussion on the Assignment 3 Canvas page).

**1) In Salt Lake City, is the seventh day of the local KSL weather forecast more or less accurate than the seventh day of the forecast provided by the U. S. National Weather Service?**

##Methodology
To answer Question 1, Salt Lake City data from the KSL weather forecast of high and low temperatures between the days of March 12, 2022 and April 9, 2022 will be compared to the actual Tmax and Tmin collected by the KSLC weather station at Salt Lake International Airport during the same date range. We will construct a hypothesis test with an alpha value of 0.05. The hypothesis will be tested through a Two Sample t-test since the population standard deviation of the continuous forecast for the given months is unknown. To provide more information on the accuracy of both forecasts, we will also construct 95% confidence intervals for the mean error of each forecast's seventh day prediction. Since the forecast data was collected twice a day, we will only consider the prediction from the morning of the seventh day. This will provide us with almost a full week of advance. The following is the numerical representation of the null hypothesis:

$H_0: \bar{x}_{KSLerr} - \bar{x}_{WCHNerr} >= 0$
$H_a: \bar{x}_{KSLerr} - \bar{x}_{WCHNerr} < 0$

The null hypothesis states that KSL has an equivalent or higher mean seventh day forecast error than The Weather Channel. We would reject the null hypothesis if there is significant evidence that KSL has a lower mean error than The Weather Channel. 

##Data Analysis
Since the data was collected twice a day over a series of days, there are a total of 14 predictions from KSL and 20 from Weather Channel. The data appears as follows:
```{r, echo=F}
head(KSL_Forecast)
```

The data is formatted to show the date on the left and the forecast high vs low. The number of the column indicates how many measurements from the actual day that measurement was collected. For example, X13 means the measurement was taken on the evening of the seventh day before the day on the column. For the purpose of analyzing the second day, X14 will provide the morning of the 7th day of advance. The next step will provide four 95% confidence intervals for the mean difference between seventh day predictions and actual temperature. The intervals will measure this difference for both the high and the low temperature for each weather forecast source. The forecast error will be measured as $T_{actual} - T_{predicted} = T_{error}$. 

```{r, echo=F}
KSL_hi_forecast <- KSL_Forecast$X14[c(seq(1,57,2))]
KSL_hi_forecast_diff <- SLC_Actual$TMAX - KSL_hi_forecast

KSL_lo_forecast <- KSL_Forecast$X14[c(seq(2,58,2))]
KSL_lo_forecast_diff <- SLC_Actual$TMIN - KSL_lo_forecast

SLC_hi_forecast <- SLC_Forecast$X14[c(seq(1,57,2))]
SLC_hi_forecast_diff <- SLC_Actual$TMAX - SLC_hi_forecast

SLC_lo_forecast <- SLC_Forecast$X14[c(seq(2,58,2))]
SLC_lo_forecast_diff <- SLC_Actual$TMIN - SLC_lo_forecast
```

First, the complete difference data for both services can be tested against the null hypothesis using a Welch Two Sample t-test.
```{r, echo=F}
t.test(c(KSL_lo_forecast_diff, KSL_hi_forecast_diff), c(SLC_lo_forecast_diff, SLC_hi_forecast_diff), alpha=0.05, alternative = "less")
```
In order to reject the null hypothesis, the value of p would have to be less than 0.05. Since $p = 0.6548$, we do not have evidence to reject the null hypothesis. It is likely to obtain a sample like the one we did given the assumption that The Weather Channel predicts the seventh day of the forecast at least as well as KSL.

For further analysis, each of the four data sets representing highs and lows for KSL and The Weather Channel can be analyzed using 95% confidence intervals to determine the approximate error of the seventh day of the forecast for each service. First, the data set for KSL difference in highs will be examined:
```{r, echo=F}
t.test(KSL_hi_forecast_diff, conf.level = 0.95)
```
This test indicates that we are 95% confident that, on average, the true max temperature of any given day in Salt Lake City from mid-March to mid-April is -0.05 to 3.84 degrees hotter than KSL predicted a week earlier. This provides evidence that KSL's forecast *underpredicts* the high temperature over the date range.

Next, examine the data for KSL's forecast lows.
```{r, echo=F}
t.test(KSL_lo_forecast_diff, conf.level = 0.95)
```
This test indicates that we are 95% confident that, on average, the true low temperature of any given day in Salt Lake City from mid-March to mid-April is -1.44 to 2.89 degrees hotter than KSL predicted a week earlier. This provides evidence that KSL's forecast *underpredicts* the low temperature over the date range, but this interval is centered closer to zero than the highs, so there is evidence that the predictions are not as inaccurate as they were with the highs.

Then examine the data for The Weather Channel's forecast highs.
```{r, echo=F}
t.test(SLC_hi_forecast_diff, conf.level = 0.95)
```
This test indicates that we are 95% confident that, on average, the true max temperature of any given day in Salt Lake City from mid-March to mid-April is 0.01 to 4.06 degrees hotter than The Weather Channel predicted a week earlier. This provides evidence that The Weather Channel's forecast *underpredicts* the high temperature over the date range. Because a difference of 0 is not included in the interval, we have fairly strong evidence that the forecast consistently underpredics the true high temperature.

Finally, examine the data for The Weather Channel's forecast lows.
```{r, echo=F}
t.test(SLC_lo_forecast_diff, conf.level = 0.95)
```
This test indicates that we are 95% confident that, on average, the true low temperature of any given day in Salt Lake City from mid-March to mid-April is -2.16 to 1.74 degrees hotter than The Weather Channel predicted a week earlier. This provides weak evidence that The Weather Channel overpredicts the the low temperature over the date range. Because the interval is nearly centered around zero, this evidence is extremely weak.

With each of these tests conducted, we can see that the difference between actual and forecast temperature is most extreme for The Weather Channel predicting the high temperatures, where the interval is centered around 2.03 degrees overprediction. KSL overpredicted high temperatures by an approximate margin of 1.89 degrees. For the low, the interval for KSL is centered at 0.72 degrees and the interval for The Weather Channel is centered at -0.21 degrees. Overall the real feeling of 2 degrees temperature difference in the ambient air is subjective, and may prove insignificant. With the caveats of inaccuracy in the weather forecast, these values seem to be fairly accurate even at their worst.

The confidence of these conclusions is dampened by a few factors from our data collection. First, neither our forecast samples nor our actual temperature samples are random samples. This may be acceptable for the forecast on the basis that forecasts are updated systematically. However, regarding the actual temperature high and low, the SLC Int'l weather station samples temperature on a fixed interval, so it may not capture the true temperature high or low for any given day. Determining the likelihood of this error is beyond the scope of this study.

###*2) Of the cities Salt Lake City, UT; Austin, TX; San Francisco, CA; New York, NY; and Chicago, IL; which city's 10th forecast day is, on average, closest to the actual temperature that day?*

SLC graph

```{r, echo = FALSE}
SLC_forecastMean <- (SLC_Forecast$X20[c(seq(1,57,2))]+SLC_Forecast$X19[c(seq(1,57,2))])/2
plot(c(seq(1,29,1)), SLC_Actual$TMAX, type = "l", col = "green", ylim = c(45,80))
lines(c(seq(1,29,1)), SLC_forecastMean, type = "l", col = "blue")
legend(0,80, legend=c('Forecast', 'Recorded'),col=c('blue', 'green'), lty = 1)
```

SLC confidence interval

```{r, echo = FALSE}
t.test(SLC_Actual$TMAX, SLC_forecastMean, conf.level = .95)
```

We can say that, on average, the 10th day forecast deviated from the actual temperature by -.486 degrees to 8.313 degrees.

######Note to self, the closer the confidence interval is to 0, the more acurate it is. We can say the above data is pretty unreliable and that they underestimated their temperature

SF graph

```{r, echo = FALSE}
SF_forecastMean <- (SF_Forecast$X20[c(seq(1,57,2))]+SF_Forecast$X19[c(seq(1,57,2))])/2
plot(c(seq(1,29,1)), SF_Actual$TMAX, type = "l", col = "green", ylim = c(45,85))
lines(c(seq(1,29,1)), SF_forecastMean, type = "l", col = "blue")
legend(0,80, legend=c('Forecast', 'Recorded'),col=c('blue', 'green'), lty = 1)
```

SF Confidence interval

```{r, echo = FALSE}
t.test(SF_Actual$TMAX, SF_forecastMean, conf.level = .95)
```

We can say that, on average, the 10th day forecast deviated from the actual temperature by -1.54 degrees to 4.609 degrees. Much more accurate prediction than SLC.

NY graph

```{r, echo = FALSE}
NY_forecastMean <- (NY_Forecast$X20[c(seq(1,57,2))]+NY_Forecast$X19[c(seq(1,57,2))])/2
plot(c(seq(1,29,1)), NY_Actual$TMAX, type = "l", col = "green", ylim = c(30,65))
lines(c(seq(1,29,1)), NY_forecastMean, type = "l", col = "blue")
legend(22,40, legend=c('Forecast', 'Recorded'),col=c('blue', 'green'), lty = 1)
```

NY Confidence interval

```{r, echo = FALSE}
t.test(NY_Actual$TMAX, NY_forecastMean, conf.level = .95)
```

We can say that, on average, the 10th day forecast deviated from the actual temperature by -5.407 degrees to 1.925 degrees. Much more acurate prediction than SLC, but not as acurate as SF.

CH graph

```{r, echo = FALSE}
CH_forecastMean <- (CHI_Forecast$X20[c(seq(1,57,2))]+CHI_Forecast$X19[c(seq(1,57,2))])/2
plot(c(seq(1,29,1)), CH_Actual$TMAX, type = "l", col = "green", ylim = c(25,75))
lines(c(seq(1,29,1)), CH_forecastMean, type = "l", col = "blue")
legend(22,40, legend=c('Forecast', 'Recorded'),col=c('blue', 'green'), lty = 1)
```

CH Confidence interval

```{r, echo = FALSE}
t.test(CH_Actual$TMAX, CH_forecastMean, conf.level = .95)
```

We can say that, on average, the 10th day forecast deviated from the actual temperature by -1.1796 degrees to 8.8692 degrees. This is probably the least accuratly predicted city.

ATX graph

```{r, echo = FALSE}
ATX_forecastMean <- (ATX_Forecast$X20[c(seq(1,57,2))]+ATX_Forecast$X19[c(seq(1,57,2))])/2
plot(c(seq(1,29,1)), ATX_Actual$TMAX, type = "l", col = "green", ylim = c(55,95))
lines(c(seq(1,29,1)), ATX_forecastMean, type = "l", col = "blue")
legend(22,65, legend=c('Forecast', 'Recorded'),col=c('blue', 'green'), lty = 1)
```

ATX Confidence interval

```{r, echo = FALSE}
t.test(ATX_Actual$TMAX, ATX_forecastMean, conf.level = .95)
```

We can say that, on average, the 10th day forecast deviated from the actual temperature by -3.9451 degrees to 2.7382 degrees. Which ties close with SF in accuracy.

Overall, Austin TX (ATX) has the most accurate weather as the means for both the predicted and recorded data sets match each other near identically.

###*3) Does the city with the "most accurate" forecast according to question 2 also have the least variability in temperature over the given time period?*

**ATX** 


```{r, echo = FALSE}
# Averages of the 10th Forecast for MAX and MIN

# ATX
ATX_hi_forecastMean <- (ATX_Forecast$X20[c(seq(1,40,2))]+ATX_Forecast$X19[c(seq(1,40,2))])/2
ATX_lo_forecastMean <- (ATX_Forecast$X20[c(seq(2,41,2))]+ATX_Forecast$X19[c(seq(2,41,2))])/2

# ATX Test for variance
var.test(ATX_Actual$TMAX[c(seq(1,20,1))],ATX_hi_forecastMean)
var.test(ATX_Actual$TMIN[c(seq(1,20,1))],ATX_lo_forecastMean)

plot(c(seq(1,20,1)), ATX_Actual$TMAX[c(seq(1,20,1))], type = "l", col = "red", ylim = c(20,95))
lines(c(seq(1,20,1)), ATX_hi_forecastMean, type = "c", col = "red")
lines(c(seq(1,20,1)), ATX_Actual$TMIN[c(seq(1,20,1))], type = "l", col = "blue")
lines(c(seq(1,20,1)), ATX_lo_forecastMean, type = "c", col = "blue")
legend(15,40, legend=c('Forecast MAX', 'Recorded MAX','Forecast MIN', 'Recorded MIN'),col=c('red', 'red','blue', 'blue'), lty = c(2,1,2,1), cex = 0.8)
```

**SLC** 
```{r, echo = FALSE}
# Averages of the 10th Forecast for MAX and MIN

# SLC
SLC_hi_forecastMean <- (SLC_Forecast$X20[c(seq(1,40,2))]+SLC_Forecast$X19[c(seq(1,40,2))])/2
SLC_lo_forecastMean <- (SLC_Forecast$X20[c(seq(2,41,2))]+SLC_Forecast$X19[c(seq(2,41,2))])/2

# SLC Test for variance
var.test(SLC_Actual$TMAX[c(seq(1,20,1))],SLC_hi_forecastMean)
var.test(SLC_Actual$TMIN[c(seq(1,20,1))],SLC_lo_forecastMean)

plot(c(seq(1,20,1)), SLC_Actual$TMAX[c(seq(1,20,1))], type = "l", col = "red", ylim = c(20,95))
lines(c(seq(1,20,1)), SLC_hi_forecastMean, type = "c", col = "red")
lines(c(seq(1,20,1)), SLC_Actual$TMIN[c(seq(1,20,1))], type = "l", col = "blue")
lines(c(seq(1,20,1)), SLC_lo_forecastMean, type = "c", col = "blue")
legend(15,40, legend=c('Forecast MAX', 'Recorded MAX','Forecast MIN', 'Recorded MIN'),col=c('red', 'red','blue', 'blue'), lty = c(2,1,2,1), cex = 0.8)
```

**CHI**
```{r, echo = FALSE}
# Averages of the 10th Forecast for MAX and MIN

# CHI
CHI_hi_forecastMean <- (CHI_Forecast$X20[c(seq(1,40,2))]+CHI_Forecast$X19[c(seq(1,40,2))])/2
CHI_lo_forecastMean <- (CHI_Forecast$X20[c(seq(2,41,2))]+CHI_Forecast$X19[c(seq(2,41,2))])/2

# CHI Test for variance
var.test(CH_Actual$TMAX[c(seq(1,20,1))],CHI_hi_forecastMean)
var.test(CH_Actual$TMIN[c(seq(1,20,1))],CHI_lo_forecastMean)

plot(c(seq(1,20,1)), CH_Actual$TMAX[c(seq(1,20,1))], type = "l", col = "red", ylim = c(20,95))
lines(c(seq(1,20,1)), CHI_hi_forecastMean, type = "c", col = "red")
lines(c(seq(1,20,1)), CH_Actual$TMIN[c(seq(1,20,1))], type = "l", col = "blue")
lines(c(seq(1,20,1)), CHI_lo_forecastMean, type = "c", col = "blue")
legend(15,90, legend=c('Forecast MAX', 'Recorded MAX','Forecast MIN', 'Recorded MIN'),col=c('red', 'red','blue', 'blue'), lty = c(2,1,2,1), cex = 0.8)
```


**NY**
```{r, echo = FALSE}
# Averages of the 10th Forecast for MAX and MIN

# NY
NY_hi_forecastMean <- (NY_Forecast$X20[c(seq(1,40,2))]+NY_Forecast$X19[c(seq(1,40,2))])/2
NY_lo_forecastMean <- (NY_Forecast$X20[c(seq(2,41,2))]+NY_Forecast$X19[c(seq(2,41,2))])/2

# NY Test for variance
var.test(NY_Actual$TMAX[c(seq(1,20,1))],NY_hi_forecastMean)
var.test(NY_Actual$TMIN[c(seq(1,20,1))],NY_lo_forecastMean)

plot(c(seq(1,20,1)), NY_Actual$TMAX[c(seq(1,20,1))], type = "l", col = "red", ylim = c(20,95))
lines(c(seq(1,20,1)), NY_hi_forecastMean, type = "c", col = "red")
lines(c(seq(1,20,1)), NY_Actual$TMIN[c(seq(1,20,1))], type = "l", col = "blue")
lines(c(seq(1,20,1)), NY_lo_forecastMean, type = "c", col = "blue")
legend(15,90, legend=c('Forecast MAX', 'Recorded MAX','Forecast MIN', 'Recorded MIN'),col=c('red', 'red','blue', 'blue'), lty = c(2,1,2,1), cex = 0.8)
```

## Conclusion

Summarize thoughts from analysis, as well as the data collection process, to answer the questions of the topic.