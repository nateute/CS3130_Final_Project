---
title: "Can We Really Trust the 10-Day Forecast?"
author: "Alex Baburnic, Nate Hansen, & David Vanegas"
date: "4/27/2022"
output: html_document
---

```{r setup, include=FALSE}
require(webr)
knitr::opts_chunk$set(echo = TRUE)
#Forecasts
SLC_Forecast <- read.csv("forecast_data/SLC_forecast.csv")
KSL_Forecast <- read.csv("forecast_data/KSL_forecast.csv")
NY_Forecast <- read.csv("forecast_data/NY_forecast.csv")
CHI_Forecast <- read.csv("forecast_data/CHI_forecast.csv")
ATX_Forecast <- read.csv("forecast_data/ATX_forecast.csv")
SF_Forecast <- read.csv("forecast_data/SF_forecast.csv")
#Actual temperatures
SLC_Actual <- read.csv("actualTemp_data/SLC_ActualTemp.csv")
NY_Actual <- read.csv("actualTemp_data/NY_ActualTemp.csv")
CH_Actual <- read.csv("actualTemp_data/CH_ActualTemp.csv")
ATX_Actual <- read.csv("actualTemp_data/ATX_ActualTemp.csv")
SF_Actual <- read.csv("actualTemp_data/SF_ActualTemp.csv")
```

## Introduction

Anyone who has sat through a rainy picnic knows by hard experience that the reliability of the weather forecast has measurable impact on one's life. In the modern age of technology and the "Internet of Things", humans have devised algorithms to provide themselves peace-of-mind and certainty. Yet, as they eat their soggy sandwiches under a nearby pergola, people wonder why the weather forecast said it would be clear when they planned the picnic a week ago. Is it common for the forecast to be so inconsistent, or did they just get anomalously unlucky? Does the local weather station or the national one provide more accurate forecast data? Sometimes the forecast calls for relatively warm days within a week, and when the days approach, the forecast has gotten much cooler or warmer. In the Salt Lake Valley, the temperature often changes sporadically near the spring equinox, referred to by the locals as "Utah weather". Is this oscillation really unique to Utah at this time of year? Are there other cities in the United States that seem to have more reliable forecasts? The authors will attempt to answer some of these questions with confidence using statistical analysis of weather data gathered from local and national forecasts.

#### Importance of Studying Weather Station Reliability

Reliable weather forecasts would bring greater certainty to the conditions from events as small as the family picnic to as large as community firework shows or rocket launches. Although gathering data about the reliability of the forecast will not inherently improve its reliability, this information can help people make decisions about where to live. Knowing the comparative reliability of the forecast for various cities can help organizations plan large national events and assess the risk of unpredictability. Data comparing local forecasts to national forecasts will help people determine which forecast to trust more when they seem to be disparate. In a broader sense, some believe that modern extreme weather patterns are a direct result of climate change, and therefore climate change damages weather predictability. Based on these topics of value, the following questions can be operationalized:

1) In Salt Lake City, is the 7th day of the local KSL weather forecast more accurate than the 7th day of the forecast provided by The Weather Channel?

2) Of the cities Salt Lake City, UT; Austin, TX; San Francisco, CA; New York, NY; and Chicago, IL; which city's 10th forecast day is, on average, closest to the actual temperature that day?

3) Does the city with the "most accurate" forecast according to question 2 also have the least variability in temperature over the month of March?


#### Methodology

To gather data that answers our question, we will use a variety of sources. Data for national forecasts will be gathered from The Weather Channel. Local Salt Lake City forecasts will be gathered from KSL Weather. Historical true high and low temperatures will be gathered from NOAA, where the stations gathering measurements will be located at the metropolis centers where the forecast is being calculated.

The forecast sources do not seem to disclose how often they update their forecasts. We considered collecting a random sample, but it is a little unrealistic to manually record the forecast at random times during the day. Hence, forecast data will be collected for all cities at 7am and 5pm Mountain Time from March 3, 2022 through April 9, 2022. This will provide ten days of forecast data for each day between March 12 and April 9. We will compare the 10th day of those forecasts to the actual high and low of the day as recorded by NOAA. Since NOAA data is also collected systematically and not randomly, any conclusions will be made with diminished confidence. 

To analyze the data to answer the questions, we have determined that we will conduct hypothesis tests and confidence intervals with 95% confidence. This seems reasonable given the low-stakes nature of this study. Nothing critical rides on its precision, but we want the conclusions to have good confidence. Since none of the samples were collected randomly given the systematic nature of weather data collection, these confidence levels will be further diminished.

#### Data Observations

As we were collecting the data, we noticed a few noteworthy trends. Specifically, we wish to address that the temperature in all cities experienced an upward trend because the measurements were taken around the Spring Equinox. Measurements in temperature variability will be high because they will reflect this upward trend. This can be seen most dramatically in the weather data collected between March 12 and March 17 in Chicago, Illinois. We believe that any affects of this trend are already properly accounted for within our analysis methods.

```{r}
head(CH_Actual)
```

## Data Analysis

### *1) In Salt Lake City, is the seventh day of the local KSL weather forecast more or less accurate than the seventh day of the forecast provided by The Weather Channel?*

To answer Question 1, Salt Lake City data from the KSL weather forecast of high and low temperatures between the days of March 12, 2022 and April 9, 2022 will be compared to the actual Tmax and Tmin collected by the KSLC weather station at Salt Lake International Airport during the same date range. We will construct a hypothesis test with an alpha value of 0.05. The hypothesis will be tested through a Two Sample t-test since the population standard deviation of the continuous forecast for the given months is unknown. To provide more information on the accuracy of both forecasts, we will also construct 95% confidence intervals for the mean error of each forecast's seventh day prediction. Since the forecast data was collected twice a day, we will only consider the prediction from the morning of the seventh day. This will provide us with almost a full week of advance. The following is the numerical representation of the null hypothesis:

$H_0: \bar{x}_{KSLerr} - \bar{x}_{WCHNerr} >= 0$

$H_a: \bar{x}_{KSLerr} - \bar{x}_{WCHNerr} < 0$

The forecast error will be measured as $T_{predicted} - T_{actual} = T_{error}$.The null hypothesis states that KSL has an equivalent or higher mean seventh day forecast error than The Weather Channel. We would reject the null hypothesis if there is significant evidence that KSL has a lower mean error than The Weather Channel. 

Since the data were collected twice a day over a series of days, there are a total of 14 predictions from KSL and 20 from Weather Channel. The data appear as follows:
```{r, echo=F}
head(KSL_Forecast)
```

The data are formatted to show the date on the left and the forecast high vs low. The number of the column indicates how many measurements from the actual day that measurement was collected. For example, X13 means the measurement was taken on the evening of the seventh day before the day on the column. For the purpose of analyzing the second day, X14 will provide the morning of the 7th day of advance. The next step will provide four 95% confidence intervals for the mean difference between seventh day predictions and actual temperature. The intervals will measure this difference for both the high and the low temperature for each weather forecast source. 

```{r, echo=F}
KSL_hi_forecast <- KSL_Forecast$X14[c(seq(1,57,2))]
KSL_hi_forecast_diff <- KSL_hi_forecast - SLC_Actual$TMAX
KSL_lo_forecast <- KSL_Forecast$X14[c(seq(2,58,2))]
KSL_lo_forecast_diff <- KSL_lo_forecast - SLC_Actual$TMIN
SLC_hi_forecast <- SLC_Forecast$X14[c(seq(1,57,2))]
SLC_hi_forecast_diff <- SLC_hi_forecast - SLC_Actual$TMAX
SLC_lo_forecast <- SLC_Forecast$X14[c(seq(2,58,2))]
SLC_lo_forecast_diff <- SLC_lo_forecast - SLC_Actual$TMIN 
```

First, the complete difference data for both services can be tested against the null hypothesis using a Welch Two Sample t-test.
```{r, echo=F}
t.test(c(abs(KSL_lo_forecast_diff), abs(KSL_hi_forecast_diff)), c(abs(SLC_lo_forecast_diff), abs(SLC_hi_forecast_diff)), alpha=0.05, alternative = "less")
plot(t.test(c(abs(KSL_lo_forecast_diff), abs(KSL_hi_forecast_diff)), c(abs(SLC_lo_forecast_diff), abs(SLC_hi_forecast_diff)), alpha=0.05, alternative = "less"))
```

In order to reject the null hypothesis, the value of p would have to be less than 0.05. Since $p = 0.59894$, we do not have evidence to reject the null hypothesis. It is likely to obtain a sample like the one we did given the assumption that The Weather Channel predicts the seventh day of the forecast at least as well as KSL.

For further analysis, each of the four data sets representing highs and lows for KSL and The Weather Channel can be analyzed using 95% confidence intervals to determine the approximate error of the seventh day of the forecast for each service. First, the data set for KSL difference in highs will be examined:
```{r, echo=F}
t.test(KSL_hi_forecast_diff, conf.level = 0.95)
```
This test indicates that we are 95% confident that, on average, the true max temperature of any given day in Salt Lake City from mid-March to mid-April is -0.05 to 3.84 degrees hotter than KSL predicted a week earlier. This provides evidence that KSL's forecast *underpredicts* the high temperature over the date range.

Next, examine the data for KSL's forecast lows.
```{r, echo=F}
t.test(KSL_lo_forecast_diff, conf.level = 0.95)
```
This test indicates that we are 95% confident that, on average, the true low temperature of any given day in Salt Lake City from mid-March to mid-April is -1.44 to 2.89 degrees hotter than KSL predicted a week earlier. This provides evidence that KSL's forecast *underpredicts* the low temperature over the date range, but this interval is centered closer to zero than the highs, so there is evidence that the predictions are not as inaccurate as they were with the highs.

Then examine the data for The Weather Channel's forecast highs.
```{r, echo=F}
t.test(SLC_hi_forecast_diff, conf.level = 0.95)
```
This test indicates that we are 95% confident that, on average, the true max temperature of any given day in Salt Lake City from mid-March to mid-April is 0.01 to 4.06 degrees hotter than The Weather Channel predicted a week earlier. This provides evidence that The Weather Channel's forecast *underpredicts* the high temperature over the date range. Because a difference of 0 is not included in the interval, we have fairly strong evidence that the forecast consistently underpredics the true high temperature.

Finally, examine the data for The Weather Channel's forecast lows.
```{r, echo=F}
t.test(SLC_lo_forecast_diff, conf.level = 0.95)
```
This test indicates that we are 95% confident that, on average, the true low temperature of any given day in Salt Lake City from mid-March to mid-April is -2.16 to 1.74 degrees hotter than The Weather Channel predicted a week earlier. This provides weak evidence that The Weather Channel overpredicts the the low temperature over the date range. Because the interval is nearly centered around zero, this evidence is extremely weak.

With each of these tests conducted, we can see that the difference between actual and forecast temperature is most extreme for The Weather Channel predicting the high temperatures, where the interval is centered around 2.03 degrees overprediction. KSL overpredicted high temperatures by an approximate margin of 1.89 degrees. For the low, the interval for KSL is centered at 0.72 degrees and the interval for The Weather Channel is centered at -0.21 degrees. Overall the real feeling of 2 degrees temperature difference in the ambient air is subjective, and may prove insignificant. With the caveats of inaccuracy in the weather forecast, these values seem to be fairly accurate even at their worst.

The confidence of these conclusions is dampened by a few factors from our data collection. First, neither our forecast samples nor our actual temperature samples are random samples. This may be acceptable for the forecast on the basis that forecasts are updated systematically. However, regarding the actual temperature high and low, the SLC Int'l weather station samples temperature on a fixed interval, so it may not capture the true temperature high or low for any given day. Determining the likelihood of this error is beyond the scope of this study.

### *2) Of the cities Salt Lake City, UT; Austin, TX; San Francisco, CA; New York, NY; and Chicago, IL; which city's 10th forecast day is, on average, closest to the actual temperature that day?*

To answer this question, this report uses the highs of each day recorded in our one month sample and the averaged forecasted highs of said day given a ten day forecast. Since tenth day forecasts fluctuate considerably, it was important to take two recordings of the highs of those days and average them to get the tenth day forecast. Below is data from each of the recorded cities, showing the difference between the tenth day forecast and the actual high temperatures of those days.

#### Salt Lake City

```{r, echo = FALSE}
SLC_forecastMean <- (SLC_Forecast$X20[c(seq(1,57,2))]+SLC_Forecast$X19[c(seq(1,57,2))])/2
plot(c(seq(1,29,1)), SLC_Actual$TMAX, type = "l", col = "green", ylim = c(45,80), main = "Salt Lake City, UT Forecasted Against Recorded Temperatures", ylab = "Degrees in Fahrenheit", xlab = "Days (3/12/22 - 4/9/22)")
lines(c(seq(1,29,1)), SLC_forecastMean, type = "l", col = "blue")
legend(0,80, legend=c('Forecast', 'Recorded'),col=c('blue', 'green'), lty = 1)
```

Since KSL only does seven-day forecasts, this report had to use the data given by NOAA. The graph above shows similar weather trends for both forecasted and recorded temperature highs, but there is a major difference on the 14th through 18th days, where the weather spiked much higher than what was forecasted. This could be seen as a timely outlier, though the rest of the data is so off that this outlier most likely does not change the statistical difference of the both data sets.

```{r, echo = FALSE}
t.test(SLC_Actual$TMAX, SLC_forecastMean, conf.level = .95)
```

Given the above interval, this report concludes with 95% accuracy that the 10th day forecast deviated from the actual temperature by between -0.486 to 8.313 degrees, grossly underestimating the daily highs in most cases and occasionally overestimating the highs. All being said, March proved to be a month where the ten-day forecast was substantially off from the true temperatures.

#### San Francisco

Going westward, San Francisco seemed to fair better at accurately predicting the weather ten days in advance.

```{r, echo = FALSE}
SF_forecastMean <- (SF_Forecast$X20[c(seq(1,57,2))]+SF_Forecast$X19[c(seq(1,57,2))])/2
plot(c(seq(1,29,1)), SF_Actual$TMAX, type = "l", col = "green", ylim = c(45,85), main = "San Francisco, CA Forecasted Against Recorded Temperatures", ylab = "Degrees in Fahrenheit", xlab = "Days (3/12/22 - 4/9/22)")
lines(c(seq(1,29,1)), SF_forecastMean, type = "l", col = "blue")
legend(0,80, legend=c('Forecast', 'Recorded'),col=c('blue', 'green'), lty = 1)
```

San Francisco had a couple of considerable spikes in temperature highs, but surprisingly maintained a much more accurate prediction of daily high temperatures compared to Salt Lake City.

```{r, echo = FALSE}
t.test(SF_Actual$TMAX, SF_forecastMean, conf.level = .95)
plot(t.test(SF_Actual$TMAX, SF_forecastMean, conf.level = .95))
```

On average and with the same confidence level as before, the 10th day forecast deviated from the actual temperature by -1.54 degrees to 4.609 degrees, giving a vastly more accurate prediction than Salt Lake City. 

#### New York City

New York City had some truly awful during the month of March 2022. So it may come as no surprise that ten-day forecasts there were not too accurate.

```{r, echo = FALSE}
NY_forecastMean <- (NY_Forecast$X20[c(seq(1,57,2))]+NY_Forecast$X19[c(seq(1,57,2))])/2
plot(c(seq(1,29,1)), NY_Actual$TMAX, type = "l", col = "green", ylim = c(30,65), main = "New York City, NY Forecasted Against Recorded Temperatures", ylab = "Degrees in Fahrenheit", xlab = "Days (3/12/22 - 4/9/22)")
lines(c(seq(1,29,1)), NY_forecastMean, type = "l", col = "blue")
legend(22,40, legend=c('Forecast', 'Recorded'),col=c('blue', 'green'), lty = 1)
```

That being said, the predictions were not as far off as one may think. In fact, New York Cities predictions were more accurate than even Salt Lake City.

```{r, echo = FALSE}
t.test(NY_Actual$TMAX, NY_forecastMean, conf.level = .95)
```

Given this reports 95% accuracy on data analysis, the 10th day forecast deviated from the actual temperature by -5.407 degrees to 1.925 degrees, typically overestimating the daily highs. As mentioned before this is much more accurate than Salt Lake City, though it is not as accurate as San Francisco. However, had the data been collected at another point in the year the predictions may have faired better, though that goes with all the cities studied in this report.

#### Chicago

Chicago may have exhibited the most fluctuating weather patterns of the five studied cities. Unlike the previous cities though, the ten-day predictions were quite off.

```{r, echo = FALSE}
CH_forecastMean <- (CHI_Forecast$X20[c(seq(1,57,2))]+CHI_Forecast$X19[c(seq(1,57,2))])/2
plot(c(seq(1,29,1)), CH_Actual$TMAX, type = "l", col = "green", ylim = c(25,75), main = "Chicago, IL Forecasted Against Recorded Temperatures", ylab = "Degrees in Fahrenheit", xlab = "Days (3/12/22 - 4/9/22)")
lines(c(seq(1,29,1)), CH_forecastMean, type = "l", col = "blue")
legend(22,40, legend=c('Forecast', 'Recorded'),col=c('blue', 'green'), lty = 1)
```

One could argue that this may seem a bit unfair for the "Windy City" to have such unreliable predictions given the locations tumultuous weather. However, all of the previous cities experienced some form of temperature fluctuation and managed to keep some level of accuracy.

```{r, echo = FALSE}
t.test(CH_Actual$TMAX, CH_forecastMean, conf.level = .95)
```

To be exact and in continuation of the very same accuracy level as before, the forecasted temperature of Chicago differed from the actual temperature between -1.1796 degrees to 8.8692 degrees, making Chicago the least accurately predicted city within the studied group of cities. Given the time frame alloted to the making of this report, collecting weather data during the spring may have been what caused such inaccuracy in Chicago's forecasts. 

#### Austin

The last city to be studied within the group was Austin, TX.

```{r, echo = FALSE}
ATX_forecastMean <- (ATX_Forecast$X20[c(seq(1,57,2))]+ATX_Forecast$X19[c(seq(1,57,2))])/2
plot(c(seq(1,29,1)), ATX_Actual$TMAX, type = "l", col = "green", ylim = c(55,95), main = "Austin, TX Forecasted Against Recorded Temperatures", ylab = "Degrees in Fahrenheit", xlab = "Days (3/12/22 - 4/9/22)")
lines(c(seq(1,29,1)), ATX_forecastMean, type = "l", col = "blue")
legend(22,65, legend=c('Forecast', 'Recorded'),col=c('blue', 'green'), lty = 1)
```

As seen clearly in the graph, the ten-day temperature predictions were quite accurate, even with the variability in temperature.

```{r, echo = FALSE}
t.test(ATX_Actual$TMAX, ATX_forecastMean, conf.level = .95)
plot(t.test(ATX_Actual$TMAX, ATX_forecastMean, conf.level = .95))
```

This reports concludes with a 95% accuracy that, on average, the 10th day forecast deviated from the actual temperature by -3.9451 degrees to 2.7382 degrees, which is just a bit more accurate than San Francisco. As such, Austin is the most accurate city of the selected cities that were recorded. However, variability can have noticeable impacts on the reliability of weather forecasts, as seen with the obvious example of Chicago.

### *3) Does the city with the "most accurate" forecast according to question 2 also have the least variability in temperature over the given time period?*

F-distribution is a continuous statistical distribution which arises in the testing of whether two observed samples have the same variance. It is often desirable to compare two variances rather than two averages. The analysis of variance or "ANOVA" will help us to test hypothesis with certain levels of confidence that given two samples of data, their own populations may have equal variance (For more information, please see Chapter 13 of the book: "Introductory Statistics". For the following hypothesis testing, we will compare the two variance samples as equal by  choosing the initial hypothesis $H_0$ as a proportion of variances with a of 1. As well as a null hypothesis $H_a$ is the proportion does not equal 1:

$H_0: \sigma_1^2 / \sigma_2^2 = 1$

$H_a: \sigma_1^2 / \sigma_2^2 \neq 1$


**Austin, TX** 

F-test for the city Austin, TX. For both maximum and minimum temperature values:

```{r, echo = FALSE}
# Averages of the 10th Forecast for MAX and MIN
# ATX
ATX_hi_forecastMean <- (ATX_Forecast$X20[c(seq(1,40,2))]+ATX_Forecast$X19[c(seq(1,40,2))])/2
ATX_lo_forecastMean <- (ATX_Forecast$X20[c(seq(2,41,2))]+ATX_Forecast$X19[c(seq(2,41,2))])/2
# ATX Test for variance
var.test(ATX_Actual$TMAX[c(seq(1,20,1))],ATX_hi_forecastMean)
var.test(ATX_Actual$TMIN[c(seq(1,20,1))],ATX_lo_forecastMean)
plot(var.test(ATX_Actual$TMAX[c(seq(1,20,1))],ATX_hi_forecastMean))
```

Since the F-statistic is 2.728$\sigma^2$ and lies outside of our 95% confidence interval (1.079$\sigma^2$ to 6.893$\sigma^2$), we can conclude that the variance between the actual temperature data and the 10th day forecast is not equal. The ratio of variances in the case of the maximum temperature is 2.728 and 8.057 for the minimum temperature, this means that the **actual** temperature data has higher variance than the forecast data.

```{r, echo = FALSE}
# ATX Comparison Graph
plot(c(seq(1,20,1)), ATX_Actual$TMAX[c(seq(1,20,1))], type = "l", col = "red", ylim = c(20,95), xlab = "Days", ylab = "Temperature (C)")
lines(c(seq(1,20,1)), ATX_hi_forecastMean, type = "c", col = "red")
lines(c(seq(1,20,1)), ATX_Actual$TMIN[c(seq(1,20,1))], type = "l", col = "blue")
lines(c(seq(1,20,1)), ATX_lo_forecastMean, type = "c", col = "blue")
legend(15,40, legend=c('Forecast MAX', 'Recorded MAX','Forecast MIN', 'Recorded MIN'),col=c('red', 'red','blue', 'blue'), lty = c(2,1,2,1), cex = 0.8)
```

Comparing the real temperature data with the 10th day forecast for the minimum and maximum registered values of each day, there is a high variance between the forecast and the real data.

**Salt Lake City, UT** 

F-test for the city Salt Lake City, UT. For both maximum and minimum temperature values:

```{r, echo = FALSE}
# Averages of the 10th Forecast for MAX and MIN
# SLC
SLC_hi_forecastMean <- (SLC_Forecast$X20[c(seq(1,40,2))]+SLC_Forecast$X19[c(seq(1,40,2))])/2
SLC_lo_forecastMean <- (SLC_Forecast$X20[c(seq(2,41,2))]+SLC_Forecast$X19[c(seq(2,41,2))])/2
# SLC Test for variance
var.test(SLC_Actual$TMAX[c(seq(1,20,1))],SLC_hi_forecastMean)
var.test(SLC_Actual$TMIN[c(seq(1,20,1))],SLC_lo_forecastMean)
plot(var.test(SLC_Actual$TMAX[c(seq(1,20,1))],SLC_hi_forecastMean))
```

For Salt Lake, the F-statistic is 2.408$\sigma^2$ and lies within of our 95% confidence interval (0.953$\sigma^2$ to 6.084$\sigma^2$), we can conclude that the variance between the actual temperature data and the 10th day forecast is similar. The ratio of variances in the case of the maximum temperature is 2.408 and 4.020 for the minimum temperature, this means that the **actual** temperature data has higher variance than the forecast data.

```{r, echo = FALSE}
plot(c(seq(1,20,1)), SLC_Actual$TMAX[c(seq(1,20,1))], type = "l", col = "red", ylim = c(20,95), xlab = "Days", ylab = "Temperature (C)")
lines(c(seq(1,20,1)), SLC_hi_forecastMean, type = "c", col = "red")
lines(c(seq(1,20,1)), SLC_Actual$TMIN[c(seq(1,20,1))], type = "l", col = "blue")
lines(c(seq(1,20,1)), SLC_lo_forecastMean, type = "c", col = "blue")
legend(15,40, legend=c('Forecast MAX', 'Recorded MAX','Forecast MIN', 'Recorded MIN'),col=c('red', 'red','blue', 'blue'), lty = c(2,1,2,1), cex = 0.8)
```

Comparing the real temperature data with the 10th day forecast for the minimum and maximum registered values of each day, there is a similar variance behavior between the forecast and the real data.

**Chicago, IL**

F-test for the city Chicago, IL. For both maximum and minimum temperature values:

```{r, echo = FALSE}
# Averages of the 10th Forecast for MAX and MIN
# CHI
CHI_hi_forecastMean <- (CHI_Forecast$X20[c(seq(1,40,2))]+CHI_Forecast$X19[c(seq(1,40,2))])/2
CHI_lo_forecastMean <- (CHI_Forecast$X20[c(seq(2,41,2))]+CHI_Forecast$X19[c(seq(2,41,2))])/2
# CHI Test for variance
var.test(CH_Actual$TMAX[c(seq(1,20,1))],CHI_hi_forecastMean)
var.test(CH_Actual$TMIN[c(seq(1,20,1))],CHI_lo_forecastMean)
plot(var.test(CH_Actual$TMAX[c(seq(1,20,1))],CHI_hi_forecastMean))
```

In the city of Chicago, the F-statistic is 4.745$\sigma^2$ and lies way outside of our 95% confidence interval (1.878$\sigma^2$ to 11.989$\sigma^2$), we can conclude that the variance between the actual temperature data and the 10th day forecast is very different. The ratio of variances in the case of the maximum temperature is 4.7453 and 2.803 for the minimum temperature, this means that the **actual** temperature data has higher variance than the forecast data.


```{r, echo = FALSE}
plot(c(seq(1,20,1)), CH_Actual$TMAX[c(seq(1,20,1))], type = "l", col = "red", ylim = c(20,95), xlab = "Days", ylab = "Temperature (C)")
lines(c(seq(1,20,1)), CHI_hi_forecastMean, type = "c", col = "red")
lines(c(seq(1,20,1)), CH_Actual$TMIN[c(seq(1,20,1))], type = "l", col = "blue")
lines(c(seq(1,20,1)), CHI_lo_forecastMean, type = "c", col = "blue")
legend(15,90, legend=c('Forecast MAX', 'Recorded MAX','Forecast MIN', 'Recorded MIN'),col=c('red', 'red','blue', 'blue'), lty = c(2,1,2,1), cex = 0.8)
```

Comparing the real temperature data with the 10th day forecast for the minimum and maximum registered values of each day, there is a very high variance behavior between the forecast and the real data.

**New York, NY**

F-test for the city New York, NY. For both maximum and minimum temperature values:

```{r, echo = FALSE}
# Averages of the 10th Forecast for MAX and MIN
# NY
NY_hi_forecastMean <- (NY_Forecast$X20[c(seq(1,40,2))]+NY_Forecast$X19[c(seq(1,40,2))])/2
NY_lo_forecastMean <- (NY_Forecast$X20[c(seq(2,41,2))]+NY_Forecast$X19[c(seq(2,41,2))])/2
# NY Test for variance
var.test(NY_Actual$TMAX[c(seq(1,20,1))],NY_hi_forecastMean)
var.test(NY_Actual$TMIN[c(seq(1,20,1))],NY_lo_forecastMean)
plot(var.test(NY_Actual$TMAX[c(seq(1,20,1))],NY_hi_forecastMean))
```

For New York, the F-statistic is 2.272$\sigma^2$ and lies within of our 95% confidence interval (0.899$\sigma^2$ to 5.740$\sigma^2$), we can conclude that the variance between the actual temperature data and the 10th day forecast is similar. The ratio of variances in the case of the maximum temperature is 2.272 and 2.426 for the minimum temperature, this means that the **actual** temperature data has higher variance than the forecast data.

```{r, echo = FALSE}
plot(c(seq(1,20,1)), NY_Actual$TMAX[c(seq(1,20,1))], type = "l", col = "red", ylim = c(20,95), xlab = "Days", ylab = "Temperature (C)")
lines(c(seq(1,20,1)), NY_hi_forecastMean, type = "c", col = "red")
lines(c(seq(1,20,1)), NY_Actual$TMIN[c(seq(1,20,1))], type = "l", col = "blue")
lines(c(seq(1,20,1)), NY_lo_forecastMean, type = "c", col = "blue")
legend(15,90, legend=c('Forecast MAX', 'Recorded MAX','Forecast MIN', 'Recorded MIN'),col=c('red', 'red','blue', 'blue'), lty = c(2,1,2,1), cex = 0.8)
```

Comparing the real temperature data with the 10th day forecast for the minimum and maximum registered values of each day, there is a very high variance behavior between the forecast and the real data.

Going back to the original question, we have the following conclusions: 

The cities with the highest ratio of variances is Chicago and Austin, and the cities with the lowest ratio of variances is Salt Lake City and New York. For all cities the 10th-day forecast showed a relative conservative variance compared to the actual temperature data. The F-test made for each city also corroborates the variability behavior of the 10th-day Forecast vs. Actual Temp. Data for both MAX and MIN variables.

## Conclusion

In conclusion, this study was not able to determine any strong correlation between the variability of temperature and the accuracy of the forecast, nor can it conclude that local forecast providers are more accurate than The Weather Channel. All tests and confidence intervals for test statistics were performed with 95% confidence, which is dampened because the samples were collected systematically and not randomly. Future studies on the correlation between temperature variability and prediction accuracy could perhaps last for 12 months or longer to capture all of the seasonal nuances of each place. Variance of temperature in our analysis is affected by the natural increase in spring, which is different from "variability" which would be the change from day to day. Studies comparing local forecasts to national forecasts could also include information like hourly precipitation predictions, which is something this study does not address at all. Finally, weather forecast accuracy could be compared between climates in a more measurable way to determine what specific factors impact weather prediction accuracy. The most useful conclusion of our study is that, for everyday casual purposes, both local and national forecast services give an approximately equally accurate view of the upcoming temperature.
